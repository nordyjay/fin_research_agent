# apps/chat/document_processor.py
from llama_index.core import SimpleDirectoryReader, Document
from llama_index.core.node_parser import SimpleNodeParser
from llama_index.readers.file import PDFReader, ImageReader
from pathlib import Path
import pdfplumber
import fitz  # PyMuPDF
from PIL import Image
import io
from .llamaindex_setup import get_index, get_vision_model, configure_llamaindex

class MultimodalDocumentProcessor:
    def __init__(self):
        configure_llamaindex()
        self.index = get_index()
        self.vision_model = get_vision_model()
        
    def process_pdf(self, pdf_path, broker, ticker, report_date):
        """
        Process PDF with multimodal support:
        1. Extract text
        2. Extract tables
        3. Extract images/charts
        4. Generate embeddings
        5. Store in vector DB
        """
        documents = []
        
        # Extract text using LlamaIndex PDF reader
        pdf_reader = PDFReader()
        text_docs = pdf_reader.load_data(file=Path(pdf_path))
        
        # Add metadata to each document
        for doc in text_docs:
            doc.metadata.update({
                'broker': broker,
                'ticker': ticker,
                'report_date': str(report_date),
                'source_file': str(pdf_path),
                'content_type': 'text'
            })
        documents.extend(text_docs)
        
        # Extract tables
        table_docs = self._extract_tables(pdf_path, broker, ticker, report_date)
        documents.extend(table_docs)
        
        # Extract images/charts
        image_docs = self._extract_images(pdf_path, broker, ticker, report_date)
        documents.extend(image_docs)
        
        # Parse into nodes and add to index
        parser = SimpleNodeParser.from_defaults(
            chunk_size=512,
            chunk_overlap=50,
        )
        nodes = parser.get_nodes_from_documents(documents)
        
        # Add to vector store
        self.index.insert_nodes(nodes)
        
        return len(nodes)
    
    def _extract_tables(self, pdf_path, broker, ticker, report_date):
        """Extract tables and create documents"""
        table_docs = []
        
        with pdfplumber.open(pdf_path) as pdf:
            for page_num, page in enumerate(pdf.pages):
                tables = page.extract_tables()
                
                for table_idx, table in enumerate(tables):
                    if not table or len(table) < 2:
                        continue
                    
                    # Convert table to markdown format
                    table_md = self._table_to_markdown(table)
                    
                    # Generate summary using LLM
                    summary = self._summarize_table(table_md)
                    
                    # Create document
                    doc = Document(
                        text=f"Table Summary: {summary}\n\nTable Data:\n{table_md}",
                        metadata={
                            'broker': broker,
                            'ticker': ticker,
                            'report_date': str(report_date),
                            'page_number': page_num + 1,
                            'content_type': 'table',
                            'table_index': table_idx,
                        }
                    )
                    table_docs.append(doc)
        
        return table_docs
    
    def _extract_images(self, pdf_path, broker, ticker, report_date):
        """Extract images and describe them using vision model"""
        image_docs = []
        
        pdf_document = fitz.open(pdf_path)
        
        for page_num in range(len(pdf_document)):
            page = pdf_document[page_num]
            images = page.get_images()
            
            for img_idx, img in enumerate(images):
                try:
                    xref = img[0]
                    base_image = pdf_document.extract_image(xref)
                    image_bytes = base_image["image"]
                    
                    # Save image temporarily
                    image = Image.open(io.BytesIO(image_bytes))
                    image_path = f"/app/media/extracted/{broker}_{ticker}_p{page_num+1}_img{img_idx}.png"
                    image.save(image_path)
                    
                    # Describe image using vision model
                    description = self._describe_image(image_path)
                    
                    # Create document with image description
                    doc = Document(
                        text=f"Image Description: {description}",
                        metadata={
                            'broker': broker,
                            'ticker': ticker,
                            'report_date': str(report_date),
                            'page_number': page_num + 1,
                            'content_type': 'image',
                            'image_path': image_path,
                            'image_index': img_idx,
                        }
                    )
                    image_docs.append(doc)
                    
                except Exception as e:
                    print(f"Error processing image: {e}")
                    continue
        
        pdf_document.close()
        return image_docs
    
    def _table_to_markdown(self, table):
        """Convert table to markdown format"""
        if not table:
            return ""
        
        md_lines = []
        
        # Header
        headers = table[0]
        md_lines.append("| " + " | ".join(str(h) for h in headers) + " |")
        md_lines.append("| " + " | ".join(["---"] * len(headers)) + " |")
        
        # Rows
        for row in table[1:]:
            md_lines.append("| " + " | ".join(str(cell) if cell else "" for cell in row) + " |")
        
        return "\n".join(md_lines)
    
    def _summarize_table(self, table_md):
        """Generate table summary using LLM"""
        from llama_index.core.llms import ChatMessage
        from .llamaindex_setup import get_llm
        
        llm = get_llm()
        
        prompt = f"""Analyze this table and provide a concise summary focusing on key metrics, trends, and important data points:

{table_md}

Summary:"""
        
        response = llm.complete(prompt)
        return response.text.strip()
    
    def _describe_image(self, image_path):
        """Describe image using vision model"""
        try:
            from llama_index.core.schema import ImageDocument
            
            # Create image document
            image_doc = ImageDocument(image_path=image_path)
            
            # Use vision model to describe
            prompt = """Describe this image in detail. If it's a chart or graph, explain:
1. Type of visualization (line chart, bar chart, etc.)
2. What metrics are shown
3. Key trends or patterns
4. Any notable data points or changes

Description:"""
            
            response = self.vision_model.complete(
                prompt=prompt,
                image_documents=[image_doc]
            )
            
            return response.text.strip()
            
        except Exception as e:
            return f"Image present (description failed: {str(e)})"